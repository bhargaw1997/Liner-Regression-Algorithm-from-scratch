# Liner-Regression-Algorithm-from-scratch
Liner Regression Algorith from scratch

This assignment shows how we can extend ordinary least squares regression, which uses the hypothesis class of linear regression functions, to non-linear regression functions modeled using polynomial basis functions and radial basis functions. The function we want to fit is  ğ‘¦ğ—ğ—‹ğ—ğ–¾=ğ‘“ğ—ğ—‹ğ—ğ–¾(ğ‘¥)=6(sin(ğ‘¥+2)+sin(2ğ‘¥+4)) . This is a univariate function as it has only one input variable. First, we generate synthetic input (data)  ğ‘¥ğ‘–  by sampling  ğ‘›=750  points from a uniform distribution on the interval  [âˆ’7.5,7.5] .

1. Regression with Polynomial Basis Functions.

This problem extends ordinary least squares regression, which uses the hypothesis class of linear regression functions_, to _non-linear regression functions modeled using polynomial basis functions. In order to learn nonlinear models using linear regression, we have to explicitly transform the data into a higher-dimensional space. The nonlinear hypothesis class we will consider is the set of  ğ‘‘ -degree polynomials of the form  ğ‘“(ğ‘¥)=ğ‘¤0+ğ‘¤1ğ‘¥+ğ‘¤2ğ‘¥2+...+ğ‘¤ğ‘‘ğ‘¥ğ‘‘  or a linear combination of polynomial basis function.

The monomials  {1,ğ‘¥,ğ‘¥2,...,ğ‘¥ğ‘‘}  are called basis functions, and each basis function  ğ‘¥ğ‘˜  has a corresponding weight  ğ‘¤ğ‘˜  associated with it, for all  ğ‘˜=1,...,ğ‘‘ . We transform each univariate data point  ğ‘¥ğ‘–  into into a multivariate ( ğ‘‘ -dimensional) data point via  ğœ™(ğ‘¥ğ‘–)â†’[1,ğ‘¥ğ‘–,ğ‘¥2ğ‘–,...,ğ‘¥ğ‘‘ğ‘–] . When this transformation is applied to every data point, it produces the Vandermonde matrix

2. Regression with Radial Basis Functions

In the previous case, we considered a nonlinear extension to linear regression using a linear combination of polynomial basis functions, where each basis function was introduced as a feature  ğœ™(ğ‘¥)=ğ‘¥ğ‘˜ . Now, we consider Gaussian radial basis functions of the form:

ğœ™(ğ±)=ğ‘’âˆ’ğ›¾(ğ‘¥âˆ’ğœ‡)2 ,
whose shape is defined by its center  ğœ‡  and its width  ğ›¾>0 . In the case of polynomial basis regression, the user's choice of the dimension  ğ‘‘  determined the transformation and the model. For radial basis regression, we have to contend with deciding how many radial basis functions we should have, and what their center and width parameters should be. For simplicity, let's assume that  ğ›¾=0.1  is fixed. Instead of trying to identify the number of radial basis functions or their centers, we can treat **each data point as the center of a radial basis function**.

This transformation uses radial basis functions centered around data points  ğ‘’âˆ’ğ›¾(ğ‘¥âˆ’ğ‘¥ğ‘–)2  and each basis function has a corresponding weight  ğ‘¤ğ‘–  associated with it, for all  ğ‘–=1,...,ğ‘› . We transform each univariate data point  ğ‘¥ğ‘—  into into a multivariate ( ğ‘› -dimensional) data point via  ğœ™(ğ‘¥ğ‘—)â†’[...,ğ‘’âˆ’ğ›¾(ğ‘¥ğ‘—âˆ’ğ‘¥ğ‘–)2,...] . When this transformation is applied to every data point, it produces the radial-basis kernel.

